{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md_4b5d4148",
   "metadata": {},
   "source": [
    "# Paper Tables and Figures\n- Goal: Build publication-ready summary tables from saved experiment outputs.\n- Flow: Load result files, compute formatted summaries, and export tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d564e20e",
   "metadata": {},
   "source": [
    "## Load Results\n- Step: Read required simulation and application summary files.\n- Check: Validate expected schema before table construction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed336a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "RESULTS_ROOT = Path('../results')\n",
    "SIM_ROOT = RESULTS_ROOT / 'simulations'\n",
    "APP_ROOT = RESULTS_ROOT / 'application'\n",
    "APP_SUMMARY = APP_ROOT / 'summary'\n",
    "\n",
    "print('RESULTS_ROOT:', RESULTS_ROOT.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f0bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_PATHS = {\n",
    "    # simulations\n",
    "    'sim_summary_ab_long': SIM_ROOT / 'summary_ALL.csv',\n",
    "    'sim_summary_ab_wide': SIM_ROOT / 'summary_ALL_wide.csv',\n",
    "    'sim_summary_cd_long': SIM_ROOT / 'summary_ALL_CD.csv',\n",
    "    'sim_placebo_ab_pivot': SIM_ROOT / 'summary_placebo_table.csv',\n",
    "    'sim_placebo_cd_pivot': SIM_ROOT / 'summary_placebo_table_CD.csv',\n",
    "\n",
    "    # application\n",
    "    'app_taskA_all_categories': APP_SUMMARY / 'taskA_model_metrics_all_categories.csv',\n",
    "    'app_taskA_category_summary': APP_SUMMARY / 'taskA_category_summary.csv',\n",
    "    'app_taskB_te_all_categories': APP_SUMMARY / 'taskB_placebo_te_summary_all_categories.csv',\n",
    "    'app_taskB_split_all_categories': APP_SUMMARY / 'taskB_placebo_split_metrics_all_categories.csv',\n",
    "}\n",
    "\n",
    "for k, v in TABLE_PATHS.items():\n",
    "    print(f'{k:30s} -> {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b7c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "TABLES = {name: load_csv(path) for name, path in TABLE_PATHS.items()}\n",
    "\n",
    "status_rows = []\n",
    "for name, df in TABLES.items():\n",
    "    status_rows.append({\n",
    "        'table': name,\n",
    "        'rows': int(df.shape[0]),\n",
    "        'cols': int(df.shape[1]),\n",
    "        'columns': ', '.join(df.columns.astype(str).tolist()),\n",
    "    })\n",
    "\n",
    "status_df = pd.DataFrame(status_rows).sort_values('table').reset_index(drop=True)\n",
    "display(status_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c4790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick preview\n",
    "for name in sorted(TABLES):\n",
    "    print(f'\\n=== {name} ===')\n",
    "    display(TABLES[name].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45452ec9",
   "metadata": {},
   "source": [
    "## Setup\n- Step: Define output directories and helper formatting functions.\n- Output: Reusable utilities for table generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de57f67f",
   "metadata": {},
   "source": [
    "## Simulation Summary Tables\n- Step: Build model-comparison tables from simulation summaries.\n- Save: CSV and formatted outputs for reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782949c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "OUT_SIM_SUMMARY = RESULTS_ROOT / 'summary_simulations'\n",
    "OUT_SIM_SUMMARY.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Base fit summaries (already mean/std over seeds) from combined A/B + C/D\n",
    "fit_ab = TABLES['sim_summary_ab_long'].copy()\n",
    "fit_cd = TABLES['sim_summary_cd_long'].copy()\n",
    "fit_all = pd.concat([fit_ab, fit_cd], ignore_index=True)\n",
    "fit_all = fit_all[(fit_all['task'] == 'fit') & (fit_all['effect_type'].isin(['alpha', 'beta']))].copy()\n",
    "\n",
    "# 2) Compute MAE for fit from per-seed fit tables (not in existing fit summary CSVs)\n",
    "def _seed_fit_path(scenario: str, seed: int) -> Path:\n",
    "    return SIM_ROOT / scenario / str(seed) / f'scen{scenario}_fit_seed{seed}.csv'\n",
    "\n",
    "mae_rows = []\n",
    "for scenario in ['A', 'B', 'C', 'D']:\n",
    "    for seed in [0, 1, 2, 3, 4]:\n",
    "        fp = _seed_fit_path(scenario, seed)\n",
    "        if not fp.exists():\n",
    "            continue\n",
    "        d = pd.read_csv(fp)\n",
    "        d = d[d['effect_type'].isin(['alpha', 'beta'])].copy()\n",
    "        d['ae'] = (d['mean'] - d['truth']).abs()\n",
    "        g = d.groupby(['model', 'effect_type'], as_index=False)['ae'].mean()\n",
    "        g = g.rename(columns={'ae': 'value', 'effect_type': 'effect_type'})\n",
    "        g['scenario'] = scenario\n",
    "        g['task'] = 'fit'\n",
    "        g['metric'] = 'MAE'\n",
    "        g['seed'] = seed\n",
    "        mae_rows.append(g[['scenario', 'task', 'model', 'effect_type', 'metric', 'value', 'seed']])\n",
    "\n",
    "if mae_rows:\n",
    "    mae_seed = pd.concat(mae_rows, ignore_index=True)\n",
    "    mae_summary = mae_seed.groupby(['scenario', 'task', 'model', 'effect_type', 'metric'], as_index=False)['value'].agg(mean='mean', std='std')\n",
    "    fit_all = pd.concat([fit_all, mae_summary], ignore_index=True)\n",
    "\n",
    "# 3) Keep requested metrics and format mean/std strings\n",
    "wanted_metrics = ['MAE', 'RMSE', 'Coverage', 'CI_Width']\n",
    "tab1 = fit_all[fit_all['metric'].isin(wanted_metrics)].copy()\n",
    "\n",
    "tab1['metric_stat'] = tab1['metric'].astype(str) + '_mean_std'\n",
    "tab1['value'] = tab1.apply(lambda r: f\"{r['mean']:.4f} ({r['std']:.4f})\", axis=1)\n",
    "\n",
    "tab1_wide = (\n",
    "    tab1.pivot_table(\n",
    "        index=['model', 'scenario', 'effect_type'],\n",
    "        columns='metric_stat',\n",
    "        values='value',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# enforce order\n",
    "model_order = ['FE+AR', 'GP-CP', 'GP-CP-Extended']\n",
    "scenario_order = ['A', 'B', 'C', 'D']\n",
    "effect_order = ['alpha', 'beta']\n",
    "\n",
    "tab1_wide['model'] = pd.Categorical(tab1_wide['model'], categories=model_order, ordered=True)\n",
    "tab1_wide['scenario'] = pd.Categorical(tab1_wide['scenario'], categories=scenario_order, ordered=True)\n",
    "tab1_wide['effect_type'] = pd.Categorical(tab1_wide['effect_type'], categories=effect_order, ordered=True)\n",
    "\n",
    "tab1_wide = tab1_wide.sort_values(['model', 'scenario', 'effect_type']).reset_index(drop=True)\n",
    "\n",
    "# 4) Print subtables by model (rows: scenario x effect)\n",
    "subtables = {}\n",
    "for m in model_order:\n",
    "    sub = tab1_wide[tab1_wide['model'] == m].copy()\n",
    "    sub = sub.drop(columns=['model'])\n",
    "    subtables[m] = sub\n",
    "    print(f\"\\n=== Table 1 Subtable: {m} ===\")\n",
    "    display(sub)\n",
    "\n",
    "# 5) Save outputs\n",
    "(tab1_wide).to_csv(OUT_SIM_SUMMARY / 'table1_model_fitting_comparison_all_models.csv', index=False)\n",
    "for m, sub in subtables.items():\n",
    "    safe = m.replace('+', 'plus').replace('-', '_').lower()\n",
    "    sub.to_csv(OUT_SIM_SUMMARY / f'table1_model_fitting_comparison_{safe}.csv', index=False)\n",
    "\n",
    "print('Wrote:', OUT_SIM_SUMMARY / 'table1_model_fitting_comparison_all_models.csv')\n",
    "for m in model_order:\n",
    "    safe = m.replace('+', 'plus').replace('-', '_').lower()\n",
    "    print('Wrote:', OUT_SIM_SUMMARY / f'table1_model_fitting_comparison_{safe}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d976fd2e",
   "metadata": {},
   "source": [
    "### Extrapolation Table\n- Step: Summarize extrapolation performance metrics by model.\n- Save: Final extrapolation comparison table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2507196",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_SIM_SUMMARY = RESULTS_ROOT / 'summary_simulations'\n",
    "OUT_SIM_SUMMARY.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sim_ab = TABLES['sim_summary_ab_long'].copy()\n",
    "sim_cd = TABLES['sim_summary_cd_long'].copy()\n",
    "sim_all = pd.concat([sim_ab, sim_cd], ignore_index=True)\n",
    "\n",
    "ex = sim_all[(sim_all['task'] == 'extrapolation') & (sim_all['effect_type'] == 'beta')].copy()\n",
    "ex = ex[ex['metric'].isin(['MAE', 'RMSE', 'Coverage', 'CI_Width'])].copy()\n",
    "\n",
    "ex['metric_col'] = ex['metric'].astype(str) + '_mean_std'\n",
    "ex['value'] = ex.apply(lambda r: f\"{r['mean']:.4f} ({r['std']:.4f})\", axis=1)\n",
    "\n",
    "table2 = (\n",
    "    ex.pivot_table(\n",
    "        index=['scenario', 'model'],\n",
    "        columns='metric_col',\n",
    "        values='value',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "scenario_order = ['A', 'B', 'C', 'D']\n",
    "model_order = ['FE+AR', 'GP-CP', 'GP-CP-Extended']\n",
    "\n",
    "table2['scenario'] = pd.Categorical(table2['scenario'], scenario_order, ordered=True)\n",
    "table2['model'] = pd.Categorical(table2['model'], model_order, ordered=True)\n",
    "table2 = table2.sort_values(['scenario', 'model']).reset_index(drop=True)\n",
    "\n",
    "print('Table 2 preview:')\n",
    "display(table2)\n",
    "\n",
    "table2.to_csv(OUT_SIM_SUMMARY / 'table2_extrapolation_comparison_all_models.csv', index=False)\n",
    "print('Wrote:', OUT_SIM_SUMMARY / 'table2_extrapolation_comparison_all_models.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de5b941",
   "metadata": {},
   "source": [
    "### Placebo Table\n- Step: Summarize placebo metrics by model.\n- Save: Final placebo comparison table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73a3996",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_ab = TABLES['sim_summary_ab_long'].copy()\n",
    "sim_cd = TABLES['sim_summary_cd_long'].copy()\n",
    "sim_all = pd.concat([sim_ab, sim_cd], ignore_index=True)\n",
    "\n",
    "pl = sim_all[sim_all['task'] == 'placebo_te'].copy()\n",
    "\n",
    "metric_map = {\n",
    "    'tau_mae_avg_over_cohorts': ('tau_t', 'MAE'),\n",
    "    'tau_rmse_avg_over_cohorts': ('tau_t', 'RMSE'),\n",
    "    'tau_coverage_avg_over_cohorts': ('tau_t', 'Coverage'),\n",
    "    'tau_ci_width_avg_over_cohorts': ('tau_t', 'CI_Width'),\n",
    "    'att_mae_vs_zero': ('ATT', 'MAE'),\n",
    "    'att_rmse_vs_zero': ('ATT', 'RMSE'),\n",
    "    'att_coverage_vs_zero': ('ATT', 'Coverage'),\n",
    "    'att_ci_width_avg_over_t': ('ATT', 'CI_Width'),\n",
    "}\n",
    "\n",
    "pl = pl[pl['metric'].isin(metric_map.keys())].copy()\n",
    "pl['estimand'] = pl['metric'].map(lambda x: metric_map[x][0])\n",
    "pl['metric_std'] = pl['metric'].map(lambda x: metric_map[x][1])\n",
    "pl['metric_col'] = pl['metric_std'].astype(str) + '_mean_std'\n",
    "pl['value'] = pl.apply(lambda r: f\"{r['mean']:.4f} ({r['std']:.4f})\", axis=1)\n",
    "\n",
    "table3 = (\n",
    "    pl.pivot_table(\n",
    "        index=['scenario', 'estimand', 'model'],\n",
    "        columns='metric_col',\n",
    "        values='value',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "scenario_order = ['A', 'B', 'C', 'D']\n",
    "estimand_order = ['tau_t', 'ATT']\n",
    "model_order = ['FE+AR', 'GP-CP', 'GP-CP-Extended']\n",
    "\n",
    "table3['scenario'] = pd.Categorical(table3['scenario'], scenario_order, ordered=True)\n",
    "table3['estimand'] = pd.Categorical(table3['estimand'], estimand_order, ordered=True)\n",
    "table3['model'] = pd.Categorical(table3['model'], model_order, ordered=True)\n",
    "table3 = table3.sort_values(['scenario', 'estimand', 'model']).reset_index(drop=True)\n",
    "\n",
    "print('Table 3 preview:')\n",
    "display(table3)\n",
    "\n",
    "table3.to_csv(OUT_SIM_SUMMARY / 'table3_placebo_comparison_all_models.csv', index=False)\n",
    "print('Wrote:', OUT_SIM_SUMMARY / 'table3_placebo_comparison_all_models.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c80bf31",
   "metadata": {},
   "source": [
    "## Application Summary Tables\n- Step: Build real-data summary tables across categories.\n- Save: Final application comparison outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463231db",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_APP_SUMMARY = RESULTS_ROOT / 'summary_applications'\n",
    "OUT_APP_SUMMARY.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "app_fit = TABLES['app_taskA_all_categories'].copy()\n",
    "\n",
    "cat_map = {\n",
    "    'gym_instore': 'Gym Instore',\n",
    "    'mealkit': 'Mealkit',\n",
    "    'streaming': 'Streaming Services',\n",
    "}\n",
    "\n",
    "table4 = app_fit[[\n",
    "    'category', 'model', 'mae_fit', 'rmse_fit', 'coverage', 'ci_width_mean'\n",
    "]].copy()\n",
    "\n",
    "table4['category'] = table4['category'].map(cat_map).fillna(table4['category'])\n",
    "\n",
    "table4 = table4.rename(columns={\n",
    "    'category': 'Category',\n",
    "    'model': 'Model',\n",
    "    'mae_fit': 'MAE',\n",
    "    'rmse_fit': 'RMSE',\n",
    "    'coverage': 'Coverage',\n",
    "    'ci_width_mean': 'CI_Width',\n",
    "})\n",
    "\n",
    "cat_order = ['Gym Instore', 'Mealkit', 'Streaming Services']\n",
    "model_order = ['FE+AR', 'GP-CP', 'GP-CP-Extended']\n",
    "\n",
    "table4['Category'] = pd.Categorical(table4['Category'], categories=cat_order, ordered=True)\n",
    "table4['Model'] = pd.Categorical(table4['Model'], categories=model_order, ordered=True)\n",
    "\n",
    "table4 = table4.sort_values(['Category', 'Model']).reset_index(drop=True)\n",
    "\n",
    "for c in ['MAE', 'RMSE', 'Coverage', 'CI_Width']:\n",
    "    table4[c] = table4[c].astype(float)\n",
    "\n",
    "print('Table 4 preview:')\n",
    "display(table4)\n",
    "\n",
    "out_csv = OUT_APP_SUMMARY / 'table4_application_full_fitting_comparison.csv'\n",
    "table4.to_csv(out_csv, index=False)\n",
    "print('Wrote:', out_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
